{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import imageio\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train import load_video, load_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "import random\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_videos_grid(videos: torch.Tensor, path: str, rescale=False, n_rows=4, fps=8):\n",
    "    videos = rearrange(videos, \"b c t h w -> t b c h w\")\n",
    "    outputs = []\n",
    "    for x in videos:\n",
    "        x = torchvision.utils.make_grid(x, nrow=n_rows)\n",
    "        x = x.transpose(0, 1).transpose(1, 2).squeeze(-1)\n",
    "        if rescale:\n",
    "            x = (x + 1.0) / 2.0  # -1,1 -> 0,1\n",
    "        x = (x * 255).numpy().astype(np.uint8)\n",
    "        outputs.append(x)\n",
    "\n",
    "    # os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    imageio.mimsave(path, outputs, duration=1000 * (1 / fps), loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'st_attn': False} were passed to VideoInpaintingModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 9\n",
      "_IncompatibleKeys(missing_keys=['down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'down_blocks.0.attentions.0.transformer_blocks.0.norm_temp.weight', 'down_blocks.0.attentions.0.transformer_blocks.0.norm_temp.bias', 'down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'down_blocks.0.attentions.1.transformer_blocks.0.norm_temp.weight', 'down_blocks.0.attentions.1.transformer_blocks.0.norm_temp.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'down_blocks.1.attentions.0.transformer_blocks.0.norm_temp.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm_temp.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'down_blocks.1.attentions.1.transformer_blocks.0.norm_temp.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm_temp.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'down_blocks.2.attentions.0.transformer_blocks.0.norm_temp.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm_temp.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'down_blocks.2.attentions.1.transformer_blocks.0.norm_temp.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm_temp.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'up_blocks.1.attentions.0.transformer_blocks.0.norm_temp.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm_temp.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'up_blocks.1.attentions.1.transformer_blocks.0.norm_temp.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm_temp.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_q.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_k.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_v.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_out.0.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_out.0.bias', 'up_blocks.1.attentions.2.transformer_blocks.0.norm_temp.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm_temp.bias', 'up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'up_blocks.2.attentions.0.transformer_blocks.0.norm_temp.weight', 'up_blocks.2.attentions.0.transformer_blocks.0.norm_temp.bias', 'up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'up_blocks.2.attentions.1.transformer_blocks.0.norm_temp.weight', 'up_blocks.2.attentions.1.transformer_blocks.0.norm_temp.bias', 'up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_q.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_k.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_v.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_out.0.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_out.0.bias', 'up_blocks.2.attentions.2.transformer_blocks.0.norm_temp.weight', 'up_blocks.2.attentions.2.transformer_blocks.0.norm_temp.bias', 'up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'up_blocks.3.attentions.0.transformer_blocks.0.norm_temp.weight', 'up_blocks.3.attentions.0.transformer_blocks.0.norm_temp.bias', 'up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'up_blocks.3.attentions.1.transformer_blocks.0.norm_temp.weight', 'up_blocks.3.attentions.1.transformer_blocks.0.norm_temp.bias', 'up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_q.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_k.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_v.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_out.0.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_out.0.bias', 'up_blocks.3.attentions.2.transformer_blocks.0.norm_temp.weight', 'up_blocks.3.attentions.2.transformer_blocks.0.norm_temp.bias', 'mid_block.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'mid_block.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'mid_block.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'mid_block.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'mid_block.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'mid_block.attentions.0.transformer_blocks.0.norm_temp.weight', 'mid_block.attentions.0.transformer_blocks.0.norm_temp.bias'], unexpected_keys=[])\n",
      "conv_in.weight\n",
      "conv_in.bias\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_k.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm_temp.weight\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm_temp.bias\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_k.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm_temp.weight\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm_temp.bias\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_k.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm_temp.weight\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm_temp.bias\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_k.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm_temp.weight\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm_temp.bias\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_k.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm_temp.weight\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm_temp.bias\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_k.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm_temp.weight\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm_temp.bias\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_k.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm_temp.weight\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm_temp.bias\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_k.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm_temp.weight\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm_temp.bias\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_k.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm_temp.weight\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm_temp.bias\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_k.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm_temp.weight\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm_temp.bias\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_k.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm_temp.weight\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm_temp.bias\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_k.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm_temp.weight\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm_temp.bias\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_k.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm_temp.weight\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm_temp.bias\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_k.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm_temp.weight\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm_temp.bias\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_q.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_k.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_v.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm_temp.weight\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm_temp.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn_temp.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn_temp.to_k.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn_temp.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.norm_temp.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.norm_temp.bias\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9373a2b38c6546d183166fae930d877a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /nas-hdd/shoubin/pretrained_model/mgie_ckpt/LLaVA-Lightning-7B-delta-v1-1 were not used when initializing LlavaLlamaForCausalLM: ['model.layers.11.mlp.up_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.mm_projector.bias', 'model.layers.28.input_layernorm.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.13.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.q_proj.weight', 'lm_head.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.mm_projector.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.4.input_layernorm.weight', 'model.norm.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.embed_tokens.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.7.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.15.mlp.up_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.26.input_layernorm.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.17.input_layernorm.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.14.input_layernorm.weight']\n",
      "- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /nas-hdd/shoubin/pretrained_model/mgie_ckpt/LLaVA-Lightning-7B-delta-v1-1 and are newly initialized: ['model.text_encoder.text_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vae.decoder.up_blocks.1.resnets.2.conv1.bias', 'model.unet.up_blocks.2.resnets.1.conv2.weight', 'model.vae.decoder.up_blocks.3.resnets.1.norm1.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.text_encoder.text_model.encoder.layers.9.self_attn.q_proj.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'model.unet.up_blocks.0.resnets.0.time_emb_proj.bias', 'model.text_encoder.text_model.encoder.layers.14.self_attn.q_proj.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'model.unet.conv_out.bias', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.text_encoder.text_model.encoder.layers.3.self_attn.k_proj.weight', 'model.unet.up_blocks.0.resnets.2.norm1.bias', 'model.vae.decoder.up_blocks.1.resnets.1.conv1.bias', 'model.vae.decoder.up_blocks.2.resnets.0.norm1.bias', 'model.unet.down_blocks.0.resnets.0.norm2.bias', 'model.vae.decoder.mid_block.resnets.1.conv2.bias', 'model.unet.down_blocks.0.resnets.1.conv1.weight', 'model.unet.up_blocks.1.attentions.1.proj_out.bias', 'model.vae.decoder.up_blocks.2.resnets.1.norm2.weight', 'model.vae.encoder.down_blocks.2.resnets.1.norm2.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.norm_temp.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.text_encoder.text_model.encoder.layers.9.mlp.fc1.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.text_encoder.text_model.encoder.layers.12.self_attn.v_proj.weight', 'model.unet.up_blocks.2.attentions.2.proj_in.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.vae.decoder.up_blocks.1.resnets.1.norm2.bias', 'model.unet.up_blocks.0.resnets.1.norm1.weight', 'model.text_encoder.text_model.final_layer_norm.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.vae.decoder.up_blocks.3.resnets.1.conv1.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'model.vae.decoder.mid_block.resnets.1.norm1.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.norm_temp.weight', 'model.unet.up_blocks.2.attentions.2.norm.bias', 'model.vae.decoder.up_blocks.0.resnets.0.norm1.bias', 'model.unet.mid_block.resnets.1.norm1.bias', 'model.unet.up_blocks.1.resnets.1.conv2.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.text_encoder.text_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vae.decoder.up_blocks.3.resnets.0.conv1.bias', 'model.text_encoder.text_model.encoder.layers.16.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vae.encoder.mid_block.resnets.0.conv2.bias', 'model.unet.down_blocks.1.attentions.0.proj_out.weight', 'model.unet.up_blocks.1.attentions.0.proj_out.weight', 'model.text_encoder.text_model.encoder.layers.5.mlp.fc2.weight', 'model.unet.up_blocks.2.resnets.2.conv_shortcut.bias', 'model.text_encoder.text_model.encoder.layers.16.layer_norm1.bias', 'model.vae.encoder.mid_block.attentions.0.to_v.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.vae.encoder.mid_block.attentions.0.to_q.bias', 'model.unet.up_blocks.2.attentions.0.proj_in.weight', 'model.unet.down_blocks.0.resnets.0.conv2.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm_temp.weight', 'model.unet.up_blocks.3.attentions.2.proj_in.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.unet.up_blocks.3.resnets.1.conv_shortcut.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.q_proj.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.q_proj.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.14.mlp.fc1.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.4.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.1.layer_norm1.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.v_proj.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.19.self_attn.out_proj.weight', 'model.unet.down_blocks.0.attentions.1.proj_out.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.vae.encoder.mid_block.resnets.1.conv2.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.v_proj.bias', 'model.unet.down_blocks.0.attentions.1.norm.weight', 'model.unet.down_blocks.3.resnets.1.conv2.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.unet.up_blocks.0.resnets.1.conv2.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.text_encoder.text_model.encoder.layers.8.self_attn.q_proj.weight', 'model.unet.down_blocks.0.attentions.0.proj_out.bias', 'model.unet.up_blocks.3.resnets.2.norm2.weight', 'model.vae.encoder.mid_block.attentions.0.to_k.weight', 'model.text_encoder.text_model.encoder.layers.18.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.8.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.5.self_attn.v_proj.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.unet.up_blocks.1.resnets.0.conv1.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_v.weight', 'model.vae.decoder.up_blocks.3.resnets.0.conv2.bias', 'model.vae.decoder.mid_block.resnets.0.norm1.weight', 'model.text_encoder.text_model.encoder.layers.5.self_attn.out_proj.weight', 'model.unet.conv_in.weight', 'model.unet.up_blocks.0.resnets.0.conv2.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm_temp.bias', 'model.text_encoder.text_model.encoder.layers.13.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.16.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.4.layer_norm2.weight', 'model.unet.down_blocks.1.attentions.0.proj_in.weight', 'model.text_encoder.text_model.encoder.layers.17.self_attn.out_proj.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.vae.encoder.down_blocks.0.resnets.1.norm1.weight', 'model.unet.up_blocks.1.attentions.0.proj_in.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.unet.up_blocks.3.resnets.1.norm1.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.vae.encoder.mid_block.resnets.0.norm1.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.text_encoder.text_model.encoder.layers.11.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.10.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.18.self_attn.q_proj.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'model.unet.up_blocks.3.resnets.2.conv2.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.norm_temp.bias', 'model.vae.decoder.mid_block.resnets.0.conv1.weight', 'model.unet.up_blocks.0.resnets.1.conv1.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.vae.encoder.down_blocks.2.resnets.1.conv1.bias', 'model.text_encoder.text_model.encoder.layers.3.mlp.fc2.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.text_encoder.text_model.encoder.layers.12.self_attn.out_proj.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.unet.up_blocks.1.resnets.1.time_emb_proj.bias', 'model.text_encoder.text_model.encoder.layers.15.self_attn.k_proj.bias', 'model.unet.up_blocks.2.resnets.2.norm1.weight', 'model.unet.up_blocks.1.upsamplers.0.conv.bias', 'model.vae.encoder.mid_block.resnets.1.conv1.weight', 'model.text_encoder.text_model.encoder.layers.1.mlp.fc2.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.norm_temp.bias', 'model.text_encoder.text_model.encoder.layers.8.layer_norm1.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'model.text_encoder.text_model.encoder.layers.6.self_attn.q_proj.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.unet.up_blocks.3.resnets.0.norm1.weight', 'model.vae.decoder.mid_block.attentions.0.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.5.self_attn.q_proj.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.unet.up_blocks.2.resnets.1.time_emb_proj.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_k.weight', 'model.unet.up_blocks.1.attentions.1.norm.bias', 'model.unet.up_blocks.1.resnets.0.conv2.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'model.text_encoder.text_model.encoder.layers.21.mlp.fc2.bias', 'model.unet.up_blocks.3.resnets.2.conv1.bias', 'model.text_encoder.text_model.encoder.layers.14.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.11.self_attn.v_proj.weight', 'model.unet.down_blocks.1.attentions.1.proj_out.bias', 'model.unet.up_blocks.1.resnets.0.conv_shortcut.weight', 'model.text_encoder.text_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vae.encoder.down_blocks.3.resnets.1.norm2.weight', 'model.vae.decoder.up_blocks.2.resnets.1.conv1.weight', 'model.text_encoder.text_model.encoder.layers.21.self_attn.v_proj.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.text_encoder.text_model.encoder.layers.15.mlp.fc1.bias', 'model.vae.decoder.mid_block.resnets.0.norm2.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.unet.up_blocks.2.attentions.2.proj_out.weight', 'model.vae.encoder.down_blocks.0.resnets.1.conv1.weight', 'model.text_encoder.text_model.encoder.layers.11.layer_norm2.bias', 'model.unet.up_blocks.3.attentions.0.proj_out.bias', 'model.text_encoder.text_model.encoder.layers.9.self_attn.k_proj.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.vae.decoder.up_blocks.1.resnets.0.norm2.weight', 'model.unet.mid_block.attentions.0.proj_in.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.text_encoder.text_model.encoder.layers.9.self_attn.v_proj.weight', 'model.unet.up_blocks.3.resnets.2.conv_shortcut.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.down_blocks.1.resnets.0.conv2.weight', 'model.vae.decoder.up_blocks.1.resnets.2.conv2.weight', 'model.unet.down_blocks.2.downsamplers.0.conv.bias', 'model.vae.encoder.down_blocks.0.resnets.0.conv2.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.unet.up_blocks.2.resnets.1.conv1.bias', 'model.vae.decoder.up_blocks.0.resnets.0.conv2.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.vae.decoder.up_blocks.3.resnets.2.conv2.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.text_encoder.text_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vae.encoder.down_blocks.0.resnets.0.conv1.bias', 'model.text_encoder.text_model.encoder.layers.4.self_attn.k_proj.weight', 'model.unet.up_blocks.3.resnets.2.norm2.bias', 'model.text_encoder.text_model.encoder.layers.19.self_attn.q_proj.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.down_blocks.1.resnets.0.norm2.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'model.text_encoder.text_model.encoder.layers.3.layer_norm2.weight', 'model.vae.decoder.up_blocks.3.resnets.2.norm1.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.unet.up_blocks.2.resnets.1.conv_shortcut.bias', 'model.text_encoder.text_model.encoder.layers.18.mlp.fc1.weight', 'model.unet.up_blocks.3.resnets.1.norm2.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.4.mlp.fc1.weight', 'model.unet.down_blocks.3.resnets.1.time_emb_proj.weight', 'model.unet.up_blocks.1.attentions.1.proj_in.bias', 'model.unet.up_blocks.2.resnets.1.norm2.weight', 'model.text_encoder.text_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vae.encoder.down_blocks.0.resnets.1.norm1.bias', 'model.unet.down_blocks.2.attentions.0.norm.weight', 'model.vae.decoder.mid_block.attentions.0.group_norm.bias', 'model.text_encoder.text_model.encoder.layers.18.self_attn.k_proj.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.3.self_attn.v_proj.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.unet.up_blocks.2.resnets.1.norm2.bias', 'model.unet.up_blocks.2.resnets.2.conv1.weight', 'model.unet.mid_block.resnets.0.time_emb_proj.weight', 'model.text_encoder.text_model.encoder.layers.17.layer_norm2.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'model.text_encoder.text_model.encoder.layers.12.self_attn.q_proj.weight', 'model.unet.up_blocks.3.attentions.2.proj_out.weight', 'model.vae.decoder.mid_block.attentions.0.group_norm.weight', 'model.text_encoder.text_model.encoder.layers.11.mlp.fc1.weight', 'model.unet.up_blocks.1.resnets.0.norm1.bias', 'model.unet.mid_block.attentions.0.norm.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.3.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.15.mlp.fc2.bias', 'model.vae.decoder.up_blocks.1.resnets.1.norm2.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.text_encoder.text_model.encoder.layers.7.self_attn.q_proj.bias', 'model.unet.down_blocks.1.resnets.0.norm1.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'model.text_encoder.text_model.encoder.layers.20.layer_norm2.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.unet.up_blocks.1.attentions.0.norm.bias', 'model.vae.encoder.mid_block.resnets.0.norm1.bias', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.vae.encoder.mid_block.attentions.0.group_norm.weight', 'model.vae.post_quant_conv.weight', 'model.unet.down_blocks.2.resnets.0.conv2.bias', 'model.text_encoder.text_model.encoder.layers.17.mlp.fc2.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.norm_temp.bias', 'model.unet.down_blocks.0.attentions.1.proj_out.bias', 'model.text_encoder.text_model.encoder.layers.8.layer_norm2.weight', 'model.vae.decoder.up_blocks.2.resnets.2.conv2.weight', 'model.vae.decoder.up_blocks.0.resnets.2.norm1.bias', 'model.unet.up_blocks.3.attentions.2.norm.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.vae.decoder.up_blocks.0.resnets.2.conv2.bias', 'model.vae.encoder.down_blocks.0.resnets.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vae.encoder.down_blocks.1.resnets.0.norm1.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'model.text_encoder.text_model.encoder.layers.10.layer_norm1.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.text_encoder.text_model.encoder.layers.9.mlp.fc2.weight', 'model.vae.encoder.down_blocks.2.resnets.0.conv_shortcut.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.unet.up_blocks.0.resnets.0.conv1.weight', 'model.unet.up_blocks.3.resnets.1.conv1.weight', 'model.text_encoder.text_model.encoder.layers.1.mlp.fc1.bias', 'model.unet.up_blocks.3.attentions.0.proj_in.weight', 'model.unet.up_blocks.1.resnets.2.conv2.bias', 'model.vae.encoder.down_blocks.3.resnets.1.norm2.bias', 'model.text_encoder.text_model.encoder.layers.9.layer_norm2.weight', 'model.unet.up_blocks.2.attentions.2.norm.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.k_proj.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.vae.encoder.down_blocks.2.resnets.1.norm2.weight', 'model.text_encoder.text_model.encoder.layers.12.layer_norm1.weight', 'model.unet.up_blocks.3.attentions.2.norm.weight', 'model.text_encoder.text_model.encoder.layers.21.self_attn.out_proj.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'model.unet.up_blocks.1.attentions.0.norm.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.vae.decoder.up_blocks.1.resnets.0.conv2.weight', 'model.vae.decoder.up_blocks.3.resnets.1.conv2.bias', 'model.vae.decoder.up_blocks.0.resnets.2.conv1.bias', 'model.text_encoder.text_model.encoder.layers.2.mlp.fc2.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.unet.up_blocks.2.resnets.0.conv_shortcut.weight', 'model.unet.up_blocks.2.resnets.2.time_emb_proj.weight', 'model.vae.decoder.up_blocks.1.resnets.1.norm1.weight', 'model.vae.decoder.up_blocks.2.upsamplers.0.conv.bias', 'model.text_encoder.text_model.encoder.layers.22.self_attn.k_proj.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.vae.encoder.mid_block.resnets.1.norm1.weight', 'model.text_encoder.text_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vae.encoder.down_blocks.3.resnets.0.norm2.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.unet.mid_block.attentions.0.norm.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.unet.up_blocks.3.resnets.2.norm1.bias', 'model.vae.encoder.down_blocks.1.resnets.1.conv1.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.text_encoder.text_model.encoder.layers.18.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.6.self_attn.v_proj.bias', 'model.unet.up_blocks.2.resnets.0.conv1.weight', 'model.vae.decoder.up_blocks.0.resnets.1.norm1.weight', 'model.text_encoder.text_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vae.decoder.up_blocks.2.resnets.0.norm2.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.unet.down_blocks.1.resnets.0.conv_shortcut.bias', 'model.vae.encoder.down_blocks.3.resnets.1.conv2.bias', 'model.text_encoder.text_model.encoder.layers.11.layer_norm1.bias', 'model.unet.up_blocks.1.attentions.1.proj_out.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.unet.down_blocks.2.resnets.1.time_emb_proj.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.up_blocks.3.resnets.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.21.self_attn.q_proj.weight', 'model.unet.up_blocks.3.resnets.2.norm1.weight', 'model.unet.up_blocks.2.attentions.2.proj_out.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.unet.up_blocks.3.resnets.1.norm1.bias', 'model.text_encoder.text_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vae.decoder.conv_in.bias', 'model.unet.up_blocks.1.resnets.2.norm2.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.22.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.11.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.1.layer_norm1.weight', 'model.vae.encoder.down_blocks.3.resnets.1.norm1.bias', 'model.unet.down_blocks.2.resnets.1.norm2.weight', 'model.unet.up_blocks.1.resnets.2.norm1.bias', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.text_encoder.text_model.embeddings.position_embedding.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.unet.mid_block.resnets.1.conv1.bias', 'model.text_encoder.text_model.encoder.layers.21.layer_norm2.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.k_proj.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.text_encoder.text_model.encoder.layers.3.mlp.fc2.weight', 'model.unet.mid_block.resnets.0.time_emb_proj.bias', 'model.text_encoder.text_model.encoder.layers.2.layer_norm1.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.text_encoder.text_model.encoder.layers.5.layer_norm2.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.text_encoder.text_model.encoder.layers.9.layer_norm1.weight', 'model.unet.up_blocks.0.resnets.0.conv1.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.18.self_attn.v_proj.bias', 'model.unet.down_blocks.3.resnets.1.norm1.bias', 'model.vae.decoder.up_blocks.1.resnets.1.conv2.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'model.unet.mid_block.resnets.1.time_emb_proj.weight', 'model.unet.up_blocks.0.resnets.2.conv2.bias', 'model.text_encoder.text_model.encoder.layers.16.self_attn.q_proj.bias', 'model.unet.down_blocks.3.resnets.1.conv1.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.vae.decoder.up_blocks.1.resnets.2.norm1.bias', 'model.unet.up_blocks.1.resnets.0.time_emb_proj.weight', 'model.text_encoder.text_model.encoder.layers.21.layer_norm2.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.unet.down_blocks.3.resnets.1.norm2.bias', 'model.text_encoder.text_model.encoder.layers.8.layer_norm2.bias', 'model.unet.up_blocks.0.resnets.2.norm2.bias', 'model.text_encoder.text_model.encoder.layers.3.self_attn.v_proj.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.3.layer_norm1.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.unet.up_blocks.2.resnets.0.norm1.bias', 'model.unet.up_blocks.0.resnets.0.norm2.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'model.vae.encoder.conv_norm_out.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.vae.decoder.mid_block.attentions.0.to_v.bias', 'model.text_encoder.text_model.encoder.layers.14.layer_norm1.bias', 'model.vae.encoder.down_blocks.1.resnets.1.conv1.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.unet.conv_norm_out.weight', 'model.vae.encoder.down_blocks.1.resnets.1.conv2.weight', 'model.unet.down_blocks.1.attentions.1.proj_in.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'model.unet.mid_block.resnets.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.2.mlp.fc1.weight', 'model.vae.encoder.conv_out.bias', 'model.unet.down_blocks.0.resnets.1.norm1.bias', 'model.vae.decoder.up_blocks.3.resnets.1.conv2.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.text_encoder.text_model.encoder.layers.19.layer_norm2.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.vae.decoder.up_blocks.0.resnets.0.conv1.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm_temp.weight', 'model.text_encoder.text_model.encoder.layers.18.layer_norm1.bias', 'model.vae.decoder.mid_block.attentions.0.to_k.bias', 'model.unet.down_blocks.2.resnets.0.time_emb_proj.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.v_proj.weight', 'model.unet.time_embedding.linear_1.weight', 'model.text_encoder.text_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vae.decoder.up_blocks.0.resnets.1.norm2.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.text_encoder.text_model.encoder.layers.1.mlp.fc2.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.0.mlp.fc2.weight', 'model.vae.encoder.down_blocks.3.resnets.1.conv1.weight', 'model.text_encoder.text_model.encoder.layers.19.layer_norm1.bias', 'model.unet.up_blocks.1.attentions.0.proj_out.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.norm_temp.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.q_proj.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.text_encoder.text_model.encoder.layers.20.self_attn.q_proj.bias', 'model.unet.up_blocks.3.attentions.1.norm.weight', 'model.text_encoder.text_model.encoder.layers.2.self_attn.q_proj.bias', 'model.unet.up_blocks.3.resnets.1.conv_shortcut.weight', 'model.text_encoder.text_model.encoder.layers.0.layer_norm2.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.text_encoder.text_model.encoder.layers.3.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.13.self_attn.q_proj.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.down_blocks.1.resnets.1.norm2.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.text_encoder.text_model.encoder.layers.10.mlp.fc1.bias', 'model.vae.decoder.up_blocks.0.resnets.0.norm1.weight', 'model.unet.down_blocks.3.resnets.0.norm1.weight', 'model.vae.encoder.mid_block.resnets.0.norm2.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.unet.up_blocks.2.upsamplers.0.conv.weight', 'model.text_encoder.text_model.encoder.layers.4.mlp.fc2.bias', 'model.unet.up_blocks.1.attentions.2.proj_out.weight', 'model.unet.mid_block.resnets.0.norm1.weight', 'model.unet.up_blocks.1.resnets.2.conv_shortcut.bias', 'model.unet.down_blocks.2.attentions.0.proj_in.bias', 'model.vae.decoder.up_blocks.1.resnets.1.conv2.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'model.text_encoder.text_model.encoder.layers.22.layer_norm2.bias', 'model.vae.decoder.up_blocks.0.upsamplers.0.conv.bias', 'model.text_encoder.text_model.encoder.layers.9.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.19.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.13.layer_norm1.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.14.self_attn.k_proj.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'model.text_encoder.text_model.encoder.layers.3.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.21.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.12.self_attn.out_proj.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_k.weight', 'model.unet.down_blocks.3.resnets.1.norm2.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vae.decoder.up_blocks.3.resnets.0.conv2.weight', 'model.vae.decoder.up_blocks.2.resnets.0.norm1.weight', 'model.text_encoder.text_model.encoder.layers.17.self_attn.q_proj.bias', 'model.unet.up_blocks.2.resnets.0.conv2.weight', 'model.text_encoder.text_model.encoder.layers.14.self_attn.out_proj.weight', 'model.unet.down_blocks.3.resnets.1.conv1.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.vae.decoder.up_blocks.3.resnets.0.norm2.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm_temp.bias', 'model.vae.encoder.down_blocks.2.resnets.0.conv_shortcut.bias', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.18.self_attn.q_proj.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'model.text_encoder.text_model.encoder.layers.4.layer_norm1.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.unet.up_blocks.0.resnets.2.conv2.weight', 'model.vae.decoder.up_blocks.2.resnets.2.norm1.bias', 'model.text_encoder.text_model.encoder.layers.5.mlp.fc1.bias', 'model.vae.decoder.mid_block.attentions.0.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.9.mlp.fc1.weight', 'model.vae.encoder.down_blocks.2.downsamplers.0.conv.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight', 'model.text_encoder.text_model.encoder.layers.12.self_attn.k_proj.weight', 'model.unet.down_blocks.0.resnets.0.conv1.bias', 'model.unet.up_blocks.0.resnets.0.norm1.bias', 'model.vae.decoder.up_blocks.3.resnets.2.norm2.weight', 'model.vae.decoder.up_blocks.3.resnets.1.norm2.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.up_blocks.2.resnets.1.conv2.bias', 'model.unet.up_blocks.3.resnets.0.conv1.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.unet.down_blocks.0.resnets.0.conv2.bias', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.unet.up_blocks.0.resnets.2.conv_shortcut.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_k.weight', 'model.unet.down_blocks.1.resnets.0.conv2.bias', 'model.text_encoder.text_model.encoder.layers.1.mlp.fc1.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.20.layer_norm1.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.unet.down_blocks.0.resnets.0.conv1.weight', 'model.text_encoder.text_model.encoder.layers.5.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.1.layer_norm2.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'model.unet.up_blocks.0.upsamplers.0.conv.weight', 'model.text_encoder.text_model.encoder.layers.6.mlp.fc2.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'model.unet.up_blocks.3.attentions.0.norm.bias', 'model.text_encoder.text_model.encoder.layers.19.mlp.fc1.weight', 'model.vae.decoder.up_blocks.1.resnets.1.conv1.weight', 'model.vae.encoder.mid_block.attentions.0.to_q.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.norm_temp.bias', 'model.unet.down_blocks.1.resnets.0.conv1.weight', 'model.text_encoder.text_model.encoder.layers.20.mlp.fc2.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.text_encoder.text_model.encoder.layers.8.mlp.fc2.weight', 'model.unet.mid_block.attentions.0.proj_in.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm_temp.bias', 'model.unet.down_blocks.3.resnets.0.conv1.weight', 'model.unet.up_blocks.2.resnets.0.time_emb_proj.weight', 'model.text_encoder.text_model.encoder.layers.15.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.20.self_attn.k_proj.bias', 'model.unet.down_blocks.0.resnets.1.time_emb_proj.weight', 'model.unet.up_blocks.3.resnets.0.conv2.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.norm_temp.weight', 'model.unet.down_blocks.1.resnets.1.conv1.bias', 'model.unet.up_blocks.1.resnets.1.time_emb_proj.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.text_encoder.text_model.encoder.layers.22.self_attn.out_proj.bias', 'model.unet.up_blocks.2.attentions.0.proj_in.bias', 'model.vae.decoder.up_blocks.2.resnets.1.conv1.bias', 'model.vae.decoder.up_blocks.1.resnets.1.norm1.bias', 'model.unet.down_blocks.1.resnets.0.time_emb_proj.bias', 'model.text_encoder.text_model.encoder.layers.14.layer_norm2.weight', 'model.unet.up_blocks.0.resnets.1.conv_shortcut.weight', 'model.unet.down_blocks.0.attentions.0.proj_in.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'model.unet.down_blocks.0.resnets.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.10.mlp.fc2.bias', 'model.unet.down_blocks.0.attentions.0.proj_in.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.unet.down_blocks.1.resnets.0.time_emb_proj.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.16.mlp.fc1.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.up_blocks.1.attentions.1.proj_in.weight', 'model.text_encoder.text_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vae.encoder.mid_block.attentions.0.to_out.0.bias', 'model.unet.mid_block.attentions.0.proj_out.bias', 'model.unet.down_blocks.2.resnets.0.conv_shortcut.bias', 'model.vae.encoder.down_blocks.2.resnets.0.conv2.bias', 'model.text_encoder.text_model.encoder.layers.15.layer_norm2.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.text_encoder.text_model.encoder.layers.0.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.11.self_attn.k_proj.weight', 'model.unet.up_blocks.1.resnets.0.time_emb_proj.bias', 'model.vae.encoder.down_blocks.3.resnets.1.conv1.bias', 'model.text_encoder.text_model.encoder.layers.2.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.11.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.12.self_attn.v_proj.bias', 'model.unet.down_blocks.0.resnets.1.conv2.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.text_encoder.text_model.encoder.layers.7.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.15.layer_norm1.bias', 'model.unet.up_blocks.2.attentions.0.norm.weight', 'model.vae.decoder.up_blocks.1.resnets.0.conv2.bias', 'model.text_encoder.text_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vae.decoder.up_blocks.0.resnets.2.norm1.weight', 'model.unet.down_blocks.2.attentions.0.proj_out.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.12.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.out_proj.bias', 'model.text_encoder.text_model.embeddings.token_embedding.weight', 'model.unet.up_blocks.0.resnets.2.conv1.bias', 'model.unet.up_blocks.1.resnets.0.norm2.bias', 'model.vae.decoder.up_blocks.0.resnets.0.conv1.weight', 'model.vae.encoder.down_blocks.3.resnets.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.10.self_attn.k_proj.bias', 'model.unet.up_blocks.1.attentions.2.norm.bias', 'model.unet.up_blocks.0.resnets.1.time_emb_proj.bias', 'model.unet.up_blocks.3.attentions.1.proj_in.weight', 'model.text_encoder.text_model.encoder.layers.5.layer_norm1.weight', 'model.unet.down_blocks.1.resnets.1.time_emb_proj.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.unet.up_blocks.2.upsamplers.0.conv.bias', 'model.text_encoder.text_model.encoder.layers.6.mlp.fc2.weight', 'model.unet.up_blocks.0.resnets.2.conv_shortcut.bias', 'model.unet.up_blocks.1.resnets.0.conv2.bias', 'model.text_encoder.text_model.encoder.layers.20.self_attn.out_proj.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.out_proj.weight', 'model.unet.up_blocks.2.resnets.1.conv_shortcut.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.vae.encoder.mid_block.resnets.1.conv1.bias', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.vae.decoder.up_blocks.3.resnets.2.conv1.weight', 'model.unet.up_blocks.2.attentions.1.proj_out.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.17.layer_norm1.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.q_proj.weight', 'model.unet.up_blocks.3.resnets.1.norm2.weight', 'model.unet.up_blocks.0.resnets.1.norm2.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.norm_temp.bias', 'model.text_encoder.text_model.encoder.layers.21.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.16.self_attn.q_proj.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'model.vae.encoder.down_blocks.1.resnets.0.conv2.weight', 'model.unet.down_blocks.2.resnets.1.conv1.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.vae.decoder.up_blocks.1.resnets.0.conv1.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.unet.down_blocks.2.attentions.1.proj_in.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.up_blocks.2.resnets.1.norm1.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.20.self_attn.v_proj.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm_temp.bias', 'model.text_encoder.text_model.encoder.layers.15.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.22.layer_norm2.weight', 'model.vae.encoder.mid_block.resnets.1.norm2.weight', 'model.unet.mid_block.resnets.0.conv2.weight', 'model.unet.up_blocks.1.resnets.1.conv1.weight', 'model.text_encoder.text_model.encoder.layers.2.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.14.mlp.fc2.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.k_proj.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.norm_temp.weight', 'model.unet.down_blocks.3.resnets.1.conv2.bias', 'model.text_encoder.text_model.encoder.layers.2.self_attn.v_proj.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.vae.decoder.mid_block.resnets.0.norm2.bias', 'model.text_encoder.text_model.encoder.layers.8.mlp.fc1.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.unet.up_blocks.0.resnets.1.norm1.bias', 'model.vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight', 'model.vae.decoder.up_blocks.1.resnets.2.conv1.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.8.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.19.layer_norm1.weight', 'model.unet.up_blocks.0.upsamplers.0.conv.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.unet.up_blocks.1.resnets.0.conv1.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.2.self_attn.q_proj.weight', 'model.unet.up_blocks.0.resnets.2.time_emb_proj.weight', 'model.text_encoder.text_model.encoder.layers.17.self_attn.k_proj.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'model.vae.encoder.mid_block.resnets.0.norm2.bias', 'model.unet.down_blocks.1.resnets.1.norm2.bias', 'model.text_encoder.text_model.encoder.layers.16.self_attn.out_proj.weight', 'model.unet.up_blocks.1.resnets.2.conv1.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.text_encoder.text_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vae.encoder.down_blocks.1.resnets.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.9.layer_norm1.bias', 'model.vae.encoder.down_blocks.2.resnets.1.conv2.bias', 'model.text_encoder.text_model.encoder.layers.12.mlp.fc1.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.vae.encoder.down_blocks.1.resnets.0.conv2.bias', 'model.unet.up_blocks.3.resnets.0.conv_shortcut.bias', 'model.unet.down_blocks.3.resnets.1.time_emb_proj.bias', 'model.text_encoder.text_model.encoder.layers.6.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.12.layer_norm2.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.text_encoder.text_model.encoder.layers.9.mlp.fc2.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.7.mlp.fc2.bias', 'model.unet.down_blocks.2.attentions.1.norm.weight', 'model.unet.up_blocks.0.resnets.0.time_emb_proj.weight', 'model.unet.up_blocks.3.resnets.2.time_emb_proj.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.text_encoder.text_model.encoder.layers.17.mlp.fc2.weight', 'model.vae.encoder.down_blocks.1.resnets.0.conv_shortcut.weight', 'model.text_encoder.text_model.encoder.layers.6.mlp.fc1.weight', 'model.unet.up_blocks.0.resnets.1.time_emb_proj.weight', 'model.unet.up_blocks.3.resnets.0.conv2.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'model.unet.up_blocks.1.resnets.1.conv2.weight', 'model.unet.down_blocks.2.attentions.0.proj_out.weight', 'model.text_encoder.text_model.encoder.layers.14.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.10.layer_norm2.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.unet.down_blocks.0.resnets.1.conv2.weight', 'model.text_encoder.text_model.encoder.layers.10.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.21.layer_norm1.weight', 'model.unet.up_blocks.1.upsamplers.0.conv.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.9.self_attn.v_proj.bias', 'model.unet.down_blocks.0.resnets.1.time_emb_proj.bias', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.v_proj.weight', 'model.unet.down_blocks.1.downsamplers.0.conv.weight', 'model.vae.encoder.down_blocks.1.resnets.1.norm1.weight', 'model.unet.up_blocks.1.resnets.1.norm2.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.vae.encoder.conv_in.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.unet.up_blocks.3.resnets.2.conv2.bias', 'model.unet.up_blocks.2.attentions.0.proj_out.weight', 'model.vae.decoder.conv_norm_out.bias', 'model.vae.encoder.mid_block.resnets.1.norm1.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.vae.quant_conv.bias', 'model.unet.down_blocks.1.attentions.0.norm.weight', 'model.text_encoder.text_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vae.encoder.down_blocks.2.resnets.0.conv2.weight', 'model.unet.up_blocks.0.resnets.2.norm2.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.unet.up_blocks.2.resnets.0.norm2.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vae.decoder.up_blocks.2.resnets.2.conv1.bias', 'model.unet.down_blocks.0.downsamplers.0.conv.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.down_blocks.0.attentions.1.norm.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vae.decoder.up_blocks.3.resnets.0.conv_shortcut.bias', 'model.text_encoder.text_model.encoder.layers.5.layer_norm2.weight', 'model.vae.decoder.up_blocks.1.resnets.2.norm2.weight', 'model.vae.decoder.up_blocks.0.resnets.1.conv2.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.text_encoder.text_model.encoder.layers.5.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vae.decoder.mid_block.resnets.0.conv2.bias', 'model.vae.encoder.down_blocks.2.resnets.1.conv2.weight', 'model.unet.up_blocks.1.resnets.2.time_emb_proj.weight', 'model.vae.encoder.down_blocks.1.resnets.1.conv2.bias', 'model.unet.up_blocks.2.resnets.0.time_emb_proj.bias', 'model.unet.down_blocks.2.resnets.1.norm1.bias', 'model.unet.mid_block.attentions.0.proj_out.weight', 'model.vae.decoder.up_blocks.3.resnets.1.norm2.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.unet.down_blocks.0.resnets.1.conv1.bias', 'model.vae.encoder.down_blocks.1.resnets.0.conv1.weight', 'model.unet.down_blocks.2.resnets.1.norm1.weight', 'model.vae.decoder.mid_block.resnets.0.conv1.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm_temp.weight', 'model.unet.down_blocks.0.resnets.1.norm2.weight', 'model.unet.down_blocks.2.resnets.0.norm1.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.21.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.19.mlp.fc2.bias', 'model.unet.down_blocks.1.attentions.1.proj_in.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.unet.up_blocks.3.attentions.1.proj_in.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.text_encoder.text_model.encoder.layers.19.layer_norm2.weight', 'model.vae.encoder.down_blocks.2.resnets.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.16.mlp.fc1.weight', 'model.unet.down_blocks.2.resnets.0.time_emb_proj.bias', 'model.vae.encoder.down_blocks.1.resnets.0.conv_shortcut.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.mid_block.resnets.0.conv1.bias', 'model.vae.decoder.up_blocks.0.resnets.0.norm2.bias', 'model.unet.down_blocks.3.resnets.0.conv1.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.vae.encoder.down_blocks.1.resnets.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.6.mlp.fc1.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'model.vae.encoder.mid_block.resnets.0.conv2.weight', 'model.vae.decoder.up_blocks.0.resnets.0.conv2.bias', 'model.text_encoder.text_model.encoder.layers.8.self_attn.k_proj.bias', 'model.unet.up_blocks.3.resnets.1.time_emb_proj.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.norm_temp.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'model.text_encoder.text_model.encoder.layers.6.layer_norm1.weight', 'model.vae.decoder.up_blocks.2.resnets.0.norm2.weight', 'model.unet.down_blocks.3.resnets.0.norm1.bias', 'model.unet.conv_out.weight', 'model.text_encoder.text_model.encoder.layers.12.mlp.fc2.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'model.vae.encoder.down_blocks.0.resnets.0.conv2.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.norm_temp.weight', 'model.text_encoder.text_model.encoder.layers.22.self_attn.q_proj.bias', 'model.unet.up_blocks.2.resnets.0.norm1.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.unet.up_blocks.2.resnets.2.norm2.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vae.encoder.down_blocks.0.downsamplers.0.conv.bias', 'model.text_encoder.text_model.encoder.layers.12.layer_norm1.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.vae.decoder.up_blocks.1.upsamplers.0.conv.weight', 'model.unet.up_blocks.2.attentions.1.proj_in.weight', 'model.text_encoder.text_model.encoder.layers.22.mlp.fc2.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.vae.decoder.up_blocks.0.upsamplers.0.conv.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.unet.down_blocks.1.attentions.0.norm.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm_temp.weight', 'model.text_encoder.text_model.encoder.layers.14.layer_norm1.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.text_encoder.text_model.encoder.layers.18.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.8.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.q_proj.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.norm_temp.bias', 'model.text_encoder.text_model.encoder.layers.7.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.4.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.5.mlp.fc2.bias', 'model.unet.down_blocks.0.attentions.0.norm.bias', 'model.text_encoder.text_model.encoder.layers.12.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.12.mlp.fc2.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.text_encoder.text_model.encoder.layers.16.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.19.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.22.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.11.layer_norm1.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.4.layer_norm1.weight', 'model.vae.decoder.up_blocks.3.resnets.0.norm1.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.text_encoder.text_model.encoder.layers.13.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.3.self_attn.out_proj.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.vae.encoder.down_blocks.0.resnets.1.conv1.bias', 'model.text_encoder.text_model.encoder.layers.10.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.18.layer_norm2.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.15.mlp.fc2.weight', 'model.unet.down_blocks.2.resnets.0.conv2.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'model.text_encoder.text_model.encoder.layers.21.self_attn.q_proj.bias', 'model.unet.down_blocks.3.resnets.0.norm2.weight', 'model.unet.down_blocks.0.resnets.0.time_emb_proj.weight', 'model.unet.up_blocks.1.attentions.2.proj_in.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.k_proj.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.unet.up_blocks.1.resnets.2.time_emb_proj.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.11.layer_norm2.weight', 'model.vae.decoder.up_blocks.3.resnets.2.conv1.bias', 'model.text_encoder.text_model.encoder.layers.15.self_attn.q_proj.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.norm_temp.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.text_encoder.text_model.encoder.layers.3.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.0.layer_norm1.bias', 'model.unet.up_blocks.0.resnets.0.norm1.weight', 'model.unet.up_blocks.0.resnets.1.conv_shortcut.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.unet.down_blocks.3.resnets.0.norm2.bias', 'model.unet.conv_in.bias', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.unet.down_blocks.2.attentions.1.proj_in.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'model.vae.encoder.down_blocks.3.resnets.1.norm1.weight', 'model.vae.decoder.up_blocks.2.resnets.0.conv2.bias', 'model.text_encoder.text_model.encoder.layers.7.mlp.fc1.bias', 'model.unet.down_blocks.1.attentions.1.norm.weight', 'model.vae.decoder.mid_block.resnets.1.norm1.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_q.weight', 'model.text_encoder.text_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vae.encoder.down_blocks.1.downsamplers.0.conv.bias', 'model.unet.down_blocks.0.resnets.0.time_emb_proj.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.unet.up_blocks.3.resnets.0.time_emb_proj.bias', 'model.unet.down_blocks.1.resnets.1.norm1.bias', 'model.vae.decoder.conv_in.weight', 'model.unet.mid_block.resnets.1.norm1.weight', 'model.text_encoder.text_model.encoder.layers.2.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.5.layer_norm1.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.vae.encoder.down_blocks.1.resnets.1.norm1.bias', 'model.unet.down_blocks.0.attentions.0.norm.weight', 'model.text_encoder.text_model.encoder.layers.6.layer_norm1.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.text_encoder.text_model.encoder.layers.15.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.18.mlp.fc2.bias', 'model.vae.decoder.up_blocks.2.resnets.2.conv1.weight', 'model.unet.down_blocks.2.resnets.0.norm2.weight', 'model.vae.encoder.down_blocks.3.resnets.0.conv1.weight', 'model.vae.encoder.down_blocks.2.downsamplers.0.conv.weight', 'model.text_encoder.text_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vae.encoder.down_blocks.2.resnets.0.norm2.weight', 'model.unet.up_blocks.3.resnets.2.time_emb_proj.bias', 'model.text_encoder.text_model.encoder.layers.11.mlp.fc1.bias', 'model.vae.encoder.down_blocks.3.resnets.0.norm1.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.out_proj.bias', 'model.unet.down_blocks.0.downsamplers.0.conv.bias', 'model.text_encoder.text_model.encoder.layers.13.self_attn.k_proj.bias', 'model.unet.down_blocks.2.attentions.1.norm.bias', 'model.unet.up_blocks.2.resnets.0.conv2.bias', 'model.unet.mid_block.resnets.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.15.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.6.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.2.self_attn.out_proj.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.text_encoder.text_model.encoder.layers.18.layer_norm1.weight', 'model.vae.encoder.down_blocks.3.resnets.0.conv1.bias', 'model.vae.decoder.up_blocks.3.resnets.2.norm2.bias', 'model.unet.up_blocks.0.resnets.2.norm1.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.down_blocks.1.attentions.0.proj_out.bias', 'model.unet.up_blocks.3.attentions.0.norm.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.q_proj.bias', 'model.unet.up_blocks.3.resnets.0.conv_shortcut.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.unet.up_blocks.3.attentions.0.proj_in.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.unet.mid_block.resnets.0.conv2.bias', 'model.vae.encoder.down_blocks.0.resnets.1.norm2.bias', 'model.unet.mid_block.resnets.1.norm2.weight', 'model.unet.up_blocks.0.resnets.0.conv2.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.unet.up_blocks.1.attentions.2.proj_out.bias', 'model.text_encoder.text_model.encoder.layers.21.self_attn.k_proj.weight', 'model.unet.down_blocks.3.resnets.0.time_emb_proj.bias', 'model.unet.up_blocks.1.resnets.1.norm1.bias', 'model.unet.down_blocks.2.resnets.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vae.decoder.up_blocks.3.resnets.1.norm1.bias', 'model.text_encoder.text_model.encoder.layers.13.layer_norm2.weight', 'model.vae.encoder.down_blocks.2.resnets.0.norm2.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.vae.encoder.down_blocks.3.resnets.0.norm2.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.unet.up_blocks.1.resnets.2.norm1.weight', 'model.vae.decoder.up_blocks.1.resnets.0.conv1.weight', 'model.vae.decoder.up_blocks.1.resnets.2.norm1.weight', 'model.unet.mid_block.resnets.0.conv1.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.unet.mid_block.resnets.1.norm2.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.text_encoder.text_model.encoder.layers.8.self_attn.k_proj.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.vae.decoder.mid_block.resnets.1.conv2.weight', 'model.unet.up_blocks.1.resnets.1.conv_shortcut.weight', 'model.vae.decoder.up_blocks.3.resnets.2.norm1.bias', 'model.unet.down_blocks.2.resnets.0.conv_shortcut.weight', 'model.vae.decoder.up_blocks.2.resnets.0.conv1.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.text_encoder.text_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vae.encoder.down_blocks.0.resnets.1.conv2.bias', 'model.unet.up_blocks.2.attentions.0.proj_out.bias', 'model.unet.down_blocks.1.resnets.0.conv1.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.text_encoder.text_model.encoder.layers.6.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.k_proj.bias', 'model.unet.down_blocks.0.attentions.1.proj_in.weight', 'model.text_encoder.text_model.encoder.layers.8.mlp.fc2.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.text_encoder.text_model.encoder.layers.4.mlp.fc2.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.unet.up_blocks.1.resnets.0.norm2.weight', 'model.unet.mid_block.resnets.1.conv1.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.2.self_attn.v_proj.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.text_encoder.text_model.encoder.layers.12.mlp.fc1.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.unet.up_blocks.0.resnets.1.conv2.weight', 'model.vae.encoder.down_blocks.2.resnets.1.norm1.bias', 'model.unet.up_blocks.2.attentions.1.norm.bias', 'model.text_encoder.text_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vae.encoder.down_blocks.2.resnets.0.norm1.weight', 'model.unet.down_blocks.3.resnets.1.norm1.weight', 'model.unet.up_blocks.1.resnets.0.norm1.weight', 'model.text_encoder.text_model.encoder.layers.6.layer_norm2.weight', 'model.unet.down_blocks.0.resnets.0.norm2.weight', 'model.vae.encoder.down_blocks.1.resnets.1.norm2.bias', 'model.text_encoder.text_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vae.decoder.up_blocks.1.resnets.0.norm1.weight', 'model.unet.up_blocks.1.attentions.2.norm.weight', 'model.text_encoder.text_model.embeddings.position_ids', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.text_encoder.text_model.encoder.layers.16.layer_norm1.weight', 'model.unet.down_blocks.3.resnets.0.conv2.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.text_encoder.text_model.encoder.layers.15.self_attn.out_proj.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.vae.encoder.down_blocks.3.resnets.1.conv2.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.text_encoder.text_model.encoder.layers.4.mlp.fc1.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.text_encoder.text_model.encoder.layers.13.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.out_proj.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.text_encoder.text_model.encoder.layers.13.mlp.fc2.bias', 'model.unet.down_blocks.1.resnets.1.norm1.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm_temp.weight', 'model.vae.encoder.down_blocks.0.downsamplers.0.conv.weight', 'model.text_encoder.text_model.encoder.layers.17.layer_norm2.bias', 'model.unet.up_blocks.3.attentions.1.proj_out.bias', 'model.unet.up_blocks.1.resnets.1.norm2.bias', 'model.unet.down_blocks.2.attentions.0.norm.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.vae.decoder.up_blocks.3.resnets.0.conv1.weight', 'model.unet.down_blocks.2.resnets.0.conv1.bias', 'model.unet.time_embedding.linear_2.bias', 'model.text_encoder.text_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vae.decoder.up_blocks.0.resnets.2.norm2.bias', 'model.text_encoder.text_model.encoder.layers.8.self_attn.out_proj.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.unet.down_blocks.0.attentions.1.proj_in.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.vae.decoder.up_blocks.2.resnets.2.norm2.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_v.weight', 'model.unet.up_blocks.2.resnets.0.conv1.bias', 'model.unet.down_blocks.1.resnets.1.conv2.weight', 'model.unet.up_blocks.2.attentions.1.proj_out.bias', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.vae.decoder.up_blocks.3.resnets.0.norm1.bias', 'model.vae.encoder.down_blocks.0.resnets.0.norm1.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.unet.down_blocks.1.resnets.1.conv1.weight', 'model.vae.decoder.up_blocks.2.resnets.0.conv2.weight', 'model.vae.decoder.mid_block.attentions.0.to_k.weight', 'model.unet.up_blocks.2.resnets.2.conv2.bias', 'model.unet.up_blocks.3.resnets.0.norm2.bias', 'model.text_encoder.text_model.encoder.layers.16.mlp.fc2.bias', 'model.unet.down_blocks.2.resnets.1.conv2.weight', 'model.unet.up_blocks.3.resnets.0.time_emb_proj.weight', 'model.unet.up_blocks.3.attentions.1.norm.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.vae.decoder.conv_out.bias', 'model.text_encoder.text_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vae.post_quant_conv.bias', 'model.vae.decoder.up_blocks.1.resnets.2.conv2.bias', 'model.vae.encoder.mid_block.resnets.0.conv1.bias', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.13.mlp.fc1.bias', 'model.unet.down_blocks.1.resnets.1.conv2.bias', 'model.text_encoder.text_model.encoder.layers.20.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.22.layer_norm1.bias', 'model.text_encoder.text_model.final_layer_norm.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm_temp.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.vae.decoder.up_blocks.2.resnets.1.norm2.bias', 'model.unet.down_blocks.1.downsamplers.0.conv.bias', 'model.text_encoder.text_model.encoder.layers.14.mlp.fc1.weight', 'model.unet.up_blocks.2.resnets.2.time_emb_proj.bias', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.text_encoder.text_model.encoder.layers.13.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.14.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.2.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.10.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.22.self_attn.k_proj.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'model.text_encoder.text_model.encoder.layers.8.self_attn.q_proj.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.vae.encoder.down_blocks.1.resnets.1.norm2.weight', 'model.vae.decoder.mid_block.resnets.0.conv2.weight', 'model.text_encoder.text_model.encoder.layers.0.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vae.decoder.up_blocks.2.resnets.0.conv1.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.out_proj.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.text_encoder.text_model.encoder.layers.3.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.k_proj.weight', 'model.unet.down_blocks.2.resnets.1.conv1.bias', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.out_proj.weight', 'model.unet.up_blocks.3.attentions.0.proj_out.weight', 'model.vae.encoder.down_blocks.1.resnets.0.norm2.bias', 'model.unet.conv_norm_out.bias', 'model.text_encoder.text_model.encoder.layers.13.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.20.layer_norm1.bias', 'model.vae.decoder.up_blocks.3.resnets.2.conv2.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.out_proj.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'model.unet.down_blocks.1.resnets.0.norm1.bias', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.text_encoder.text_model.encoder.layers.2.self_attn.k_proj.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm_temp.weight', 'model.vae.decoder.up_blocks.0.resnets.1.conv2.weight', 'model.vae.decoder.up_blocks.2.resnets.2.norm1.weight', 'model.unet.up_blocks.1.attentions.0.proj_in.weight', 'model.text_encoder.text_model.encoder.layers.13.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.16.layer_norm2.weight', 'model.unet.mid_block.resnets.1.conv2.weight', 'model.vae.decoder.conv_norm_out.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.vae.decoder.up_blocks.0.resnets.2.norm2.weight', 'model.text_encoder.text_model.encoder.layers.17.self_attn.k_proj.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.vae.encoder.mid_block.attentions.0.to_v.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'model.vae.decoder.up_blocks.2.resnets.2.norm2.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.unet.down_blocks.0.resnets.1.norm2.bias', 'model.text_encoder.text_model.encoder.layers.12.self_attn.k_proj.bias', 'model.unet.down_blocks.2.resnets.1.conv2.bias', 'model.vae.encoder.down_blocks.3.resnets.0.conv2.bias', 'model.vae.decoder.up_blocks.0.resnets.2.conv1.weight', 'model.text_encoder.text_model.encoder.layers.15.self_attn.out_proj.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.vae.encoder.mid_block.attentions.0.group_norm.bias', 'model.unet.up_blocks.3.resnets.0.conv1.bias', 'model.unet.up_blocks.3.resnets.2.conv1.weight', 'model.vae.encoder.conv_norm_out.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.7.mlp.fc1.weight', 'model.unet.up_blocks.2.resnets.2.norm1.bias', 'model.unet.down_blocks.3.resnets.0.conv2.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.unet.up_blocks.3.resnets.1.conv2.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.unet.up_blocks.2.attentions.2.proj_in.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.17.mlp.fc1.bias', 'model.unet.up_blocks.2.resnets.2.conv_shortcut.weight', 'model.vae.encoder.down_blocks.2.resnets.0.conv1.weight', 'model.vae.decoder.up_blocks.0.resnets.1.norm1.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.norm_temp.bias', 'model.unet.up_blocks.1.resnets.2.conv1.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm_temp.bias', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.unet.down_blocks.2.resnets.1.norm2.bias', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.vae.decoder.conv_out.weight', 'model.unet.down_blocks.2.attentions.1.proj_out.bias', 'model.unet.up_blocks.2.attentions.1.proj_in.bias', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_v.weight', 'model.text_encoder.text_model.encoder.layers.0.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vae.encoder.down_blocks.0.resnets.0.conv1.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.text_encoder.text_model.encoder.layers.1.self_attn.q_proj.weight', 'model.unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.vae.encoder.mid_block.attentions.0.to_k.bias', 'model.text_encoder.text_model.encoder.layers.10.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.7.mlp.fc2.weight', 'model.unet.mid_block.resnets.1.time_emb_proj.bias', 'model.vae.encoder.down_blocks.0.resnets.0.norm2.bias', 'model.vae.decoder.up_blocks.2.resnets.0.conv_shortcut.bias', 'model.text_encoder.text_model.encoder.layers.20.mlp.fc2.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.5.self_attn.k_proj.bias', 'model.unet.up_blocks.2.resnets.1.time_emb_proj.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.text_encoder.text_model.encoder.layers.11.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vae.decoder.up_blocks.2.resnets.1.norm1.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.22.self_attn.v_proj.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.16.self_attn.out_proj.bias', 'model.unet.up_blocks.2.attentions.0.norm.bias', 'model.vae.decoder.up_blocks.2.resnets.1.conv2.bias', 'model.unet.mid_block.resnets.0.norm2.bias', 'model.text_encoder.text_model.encoder.layers.19.mlp.fc1.bias', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn_temp.to_k.weight', 'model.text_encoder.text_model.encoder.layers.16.layer_norm2.bias', 'model.unet.down_blocks.2.attentions.0.proj_in.weight', 'model.unet.down_blocks.2.resnets.1.time_emb_proj.weight', 'model.text_encoder.text_model.encoder.layers.9.layer_norm2.bias', 'model.unet.up_blocks.1.attentions.2.proj_in.weight', 'model.vae.decoder.up_blocks.2.upsamplers.0.conv.weight', 'model.unet.up_blocks.1.resnets.0.conv_shortcut.bias', 'model.unet.up_blocks.1.resnets.2.norm2.bias', 'model.vae.encoder.down_blocks.2.resnets.1.norm1.weight', 'model.vae.decoder.up_blocks.0.resnets.2.conv2.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.text_encoder.text_model.encoder.layers.2.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.11.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vae.decoder.mid_block.attentions.0.to_q.weight', 'model.text_encoder.text_model.encoder.layers.18.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.18.layer_norm2.bias', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.8.self_attn.v_proj.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.text_encoder.text_model.encoder.layers.5.mlp.fc1.weight', 'model.vae.quant_conv.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.unet.down_blocks.2.resnets.0.norm2.bias', 'model.vae.decoder.up_blocks.2.resnets.1.norm1.bias', 'model.vae.decoder.up_blocks.3.resnets.0.norm2.bias', 'model.unet.time_embedding.linear_1.bias', 'model.unet.down_blocks.1.resnets.0.norm2.bias', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'model.unet.up_blocks.2.resnets.2.norm2.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'model.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_v.weight', 'model.vae.decoder.up_blocks.1.upsamplers.0.conv.bias', 'model.unet.up_blocks.0.resnets.0.conv_shortcut.weight', 'model.vae.decoder.mid_block.attentions.0.to_q.bias', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.unet.down_blocks.1.resnets.0.conv_shortcut.weight', 'model.unet.up_blocks.0.resnets.2.time_emb_proj.bias', 'model.vae.decoder.up_blocks.2.resnets.2.conv2.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm_temp.weight', 'model.vae.decoder.mid_block.resnets.1.conv1.weight', 'model.unet.up_blocks.2.resnets.1.norm1.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.unet.up_blocks.3.resnets.2.conv_shortcut.weight', 'model.unet.down_blocks.2.downsamplers.0.conv.weight', 'model.text_encoder.text_model.encoder.layers.19.self_attn.v_proj.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.text_encoder.text_model.encoder.layers.21.mlp.fc2.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.norm_temp.weight', 'model.unet.down_blocks.1.attentions.1.proj_out.weight', 'model.unet.down_blocks.0.resnets.0.norm1.weight', 'model.text_encoder.text_model.encoder.layers.20.mlp.fc1.bias', 'model.vae.decoder.up_blocks.3.resnets.1.conv1.weight', 'model.unet.up_blocks.0.resnets.1.norm2.bias', 'model.vae.decoder.up_blocks.1.resnets.0.norm1.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.vae.encoder.down_blocks.0.resnets.1.norm2.weight', 'model.vae.decoder.mid_block.resnets.0.norm1.bias', 'model.unet.up_blocks.2.resnets.0.norm2.bias', 'model.unet.up_blocks.0.resnets.1.conv1.weight', 'model.unet.up_blocks.2.resnets.2.conv2.weight', 'model.vae.decoder.up_blocks.0.resnets.1.norm2.weight', 'model.unet.up_blocks.1.resnets.1.norm1.weight', 'model.text_encoder.text_model.encoder.layers.17.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.7.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.2.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.13.mlp.fc2.weight', 'model.unet.down_blocks.2.resnets.0.conv1.weight', 'model.vae.decoder.up_blocks.0.resnets.1.conv1.bias', 'model.vae.encoder.down_blocks.1.downsamplers.0.conv.weight', 'model.unet.up_blocks.1.resnets.2.conv2.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.text_encoder.text_model.encoder.layers.17.mlp.fc1.weight', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.13.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.out_proj.bias', 'model.unet.up_blocks.2.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.unet.mid_block.resnets.1.conv2.bias', 'model.text_encoder.text_model.encoder.layers.17.layer_norm1.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_out.0.weight', 'model.unet.up_blocks.3.attentions.2.proj_out.bias', 'model.unet.up_blocks.3.attentions.2.proj_in.weight', 'model.vae.decoder.mid_block.resnets.1.norm2.bias', 'model.unet.down_blocks.2.attentions.1.proj_out.weight', 'model.text_encoder.text_model.encoder.layers.11.mlp.fc2.bias', 'model.vae.decoder.mid_block.resnets.1.conv1.bias', 'model.text_encoder.text_model.encoder.layers.14.self_attn.q_proj.weight', 'model.unet.up_blocks.0.resnets.0.conv_shortcut.bias', 'model.text_encoder.text_model.encoder.layers.14.self_attn.k_proj.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.unet.down_blocks.0.attentions.0.proj_out.weight', 'model.unet.up_blocks.3.attentions.0.transformer_blocks.0.attn_temp.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.3.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.3.layer_norm1.bias', 'model.unet.down_blocks.1.attentions.1.norm.bias', 'model.text_encoder.text_model.encoder.layers.20.self_attn.k_proj.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn_temp.to_q.weight', 'model.unet.up_blocks.3.attentions.1.proj_out.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.text_encoder.text_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vae.encoder.conv_out.weight', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.unet.up_blocks.1.resnets.2.conv_shortcut.weight', 'model.unet.up_blocks.2.resnets.1.conv1.weight', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.vae.decoder.mid_block.attentions.0.to_v.weight', 'model.vae.encoder.mid_block.attentions.0.to_out.0.weight', 'model.unet.up_blocks.2.attentions.1.norm.weight', 'model.text_encoder.text_model.encoder.layers.18.self_attn.out_proj.weight', 'model.unet.up_blocks.3.resnets.1.time_emb_proj.bias', 'model.vae.encoder.down_blocks.0.resnets.1.conv2.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.text_encoder.text_model.encoder.layers.0.mlp.fc1.weight', 'model.unet.down_blocks.0.attentions.1.transformer_blocks.0.norm_temp.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm_temp.bias', 'model.unet.up_blocks.0.resnets.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.20.layer_norm2.bias', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.attn_temp.to_v.weight', 'model.text_encoder.text_model.encoder.layers.22.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.1.layer_norm2.weight', 'model.vae.encoder.mid_block.resnets.1.norm2.bias', 'model.unet.down_blocks.1.attentions.0.proj_in.bias', 'model.vae.encoder.down_blocks.1.resnets.0.conv1.bias', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.vae.encoder.mid_block.resnets.0.conv1.weight', 'model.vae.encoder.mid_block.resnets.1.conv2.bias', 'model.unet.up_blocks.3.resnets.1.conv2.weight', 'model.vae.encoder.down_blocks.3.resnets.0.conv2.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.norm_temp.weight', 'model.text_encoder.text_model.encoder.layers.3.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.8.self_attn.v_proj.bias', 'model.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.v_proj.bias', 'model.unet.up_blocks.0.resnets.2.conv1.weight', 'model.vae.decoder.up_blocks.1.resnets.2.norm2.bias', 'model.unet.time_embedding.linear_2.weight', 'model.vae.encoder.conv_in.weight', 'model.text_encoder.text_model.encoder.layers.20.mlp.fc1.weight', 'model.unet.down_blocks.3.resnets.0.time_emb_proj.weight', 'model.unet.down_blocks.1.resnets.1.time_emb_proj.bias', 'model.vae.decoder.up_blocks.0.resnets.1.conv1.weight', 'model.unet.up_blocks.1.resnets.1.conv1.bias', 'model.text_encoder.text_model.encoder.layers.19.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.15.mlp.fc1.weight', 'model.unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'model.unet.up_blocks.2.resnets.0.conv_shortcut.bias', 'model.text_encoder.text_model.encoder.layers.22.mlp.fc1.weight', 'model.unet.down_blocks.0.resnets.1.norm1.weight', 'model.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.unet.up_blocks.3.resnets.1.conv1.bias', 'model.unet.up_blocks.2.resnets.2.conv1.bias', 'model.unet.up_blocks.3.resnets.0.norm2.weight', 'model.vae.decoder.up_blocks.1.resnets.0.norm2.bias', 'model.text_encoder.text_model.encoder.layers.9.self_attn.k_proj.weight', 'model.unet.up_blocks.1.resnets.1.conv_shortcut.bias', 'model.vae.decoder.mid_block.resnets.1.norm2.weight', 'model.text_encoder.text_model.encoder.layers.0.mlp.fc2.bias', 'model.unet.up_blocks.1.attentions.1.norm.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'model.unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.vae.decoder.up_blocks.0.resnets.0.norm2.weight', 'model.text_encoder.text_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vae.decoder.up_blocks.2.resnets.1.conv2.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.v_proj.bias', 'model.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.text_encoder.text_model.encoder.layers.9.self_attn.q_proj.bias', 'model.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn_temp.to_k.weight', 'model.unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'model.unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.vae.encoder.down_blocks.0.resnets.0.norm1.bias', 'model.text_encoder.text_model.encoder.layers.7.layer_norm2.bias', 'model.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.vae.encoder.down_blocks.2.resnets.0.conv1.bias', 'model.text_encoder.text_model.encoder.layers.22.mlp.fc2.weight', 'model.vae.encoder.down_blocks.2.resnets.1.conv1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LlavaLlamaForCausalLM.from_pretrained('/nas-hdd/shoubin/pretrained_model/mgie_ckpt/LLaVA-Lightning-7B-delta-v1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'st_attn': False} were passed to VideoInpaintingModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 9\n"
     ]
    }
   ],
   "source": [
    "from LLaVA.llava.model.video_diffusion.unet import VideoInpaintingModel\n",
    "# model.unet = VideoInpaintingModel.from_pretrained('/nas-hdd/shoubin/pretrained_model/lgvi/lgvi', subfolder='unet_trainedv3')\n",
    "# model.unet = VideoInpaintingModel.from_pretrained('/nas-hdd/shoubin/pretrained_model/stable-diffusion-2-inpainting/', subfolder='unet')\n",
    "model.unet = VideoInpaintingModel.from_pretrained('/nas-hdd/shoubin/pretrained_model/stable-diffusion-2-inpainting/', subfolder='unet_finetuned')\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_encoder.to(weight_dtype)\n",
    "model.vae.to(weight_dtype)\n",
    "model.unet.to(weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from einops import rearrange\n",
    "import random as rnd\n",
    "\n",
    "def load_mask(video_path, indices, mask_id, convert_to_box=False):\n",
    "    WIDTH = 512\n",
    "    HEIGHT = 320\n",
    "    \n",
    "    # print(video_path)\n",
    "    frame_files = list(sorted(os.listdir(video_path)))\n",
    "    frame_files = [x for x in frame_files if not x.startswith('.')]  # Excludes files like .DS_Store\n",
    "    selected_frames = [frame_files[i] for i in indices]\n",
    "    frames = []\n",
    "    \n",
    "    for frame_name in selected_frames:\n",
    "        image = Image.open(os.path.join(video_path, frame_name))\n",
    "        all_mask = np.array(image)\n",
    "        # mask = (all_mask == int(mask_id)).astype(np.uint8) * 255\n",
    "        mask = all_mask.astype(np.uint8) * 255\n",
    "        \n",
    "        if convert_to_box:\n",
    "            box_image = Image.new(\"L\", image.size, 255)\n",
    "            draw = ImageDraw.Draw(box_image)\n",
    "            # Find the bounding box of the mask\n",
    "            rows = np.any(mask, axis=1)\n",
    "            cols = np.any(mask, axis=0)\n",
    "            # box = (xmin, ymin, xmax, ymax)\n",
    "            if rows.any() and cols.any():  # Only proceed if there is at least one non-zero value\n",
    "                ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "                xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "                draw.rectangle([xmin , ymin, xmax, ymax], fill=0)\n",
    "        \n",
    "            box_image = box_image.resize((WIDTH, HEIGHT), resample=Image.BILINEAR)\n",
    "            box_np = np.array(box_image)\n",
    "            box_tensor = torch.from_numpy(box_np).float().div(255).unsqueeze(0)  # Add channel dimension\n",
    "            frames.append(box_tensor)\n",
    "        # Stack all tensors to create a batch\n",
    "            \n",
    "        else:\n",
    "            image = Image.fromarray(mask)\n",
    "            image = image.resize((WIDTH, HEIGHT), resample=Image.BILINEAR)\n",
    "            frames.append(image)\n",
    "    \n",
    "    if not convert_to_box:\n",
    "        # Stack images and convert to a tensor\n",
    "        frames = np.stack(frames, axis=2)\n",
    "        frames = torch.from_numpy(frames).permute(2, 0, 1).contiguous().unsqueeze(1)\n",
    "        frames = torch.where(frames > 0, torch.tensor(0.0), torch.tensor(1.0))\n",
    "    else:\n",
    "        frames = torch.stack(frames, dim=0)\n",
    "        frames = torch.where(frames > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "    \n",
    "    return frames\n",
    "    \n",
    "def load_video(video_path, sample_num=16, sample_type='uniform', given_index=None):\n",
    "    WIDTH = 512\n",
    "    HEIGHT = 320\n",
    "    \n",
    "    \n",
    "    frame_files = list(sorted(os.listdir(video_path)))\n",
    "    # exclude .DS_Store\n",
    "    frame_files = [x for x in frame_files if x[0]!='.']\n",
    "    # print(frame_files)\n",
    "    vlen = len(frame_files)\n",
    "\n",
    "    n_frms = min(sample_num, vlen)\n",
    "    start, end = 0, vlen\n",
    "\n",
    "    if given_index is None:\n",
    "        intervals = np.linspace(start=start, stop=end, num=n_frms + 1).astype(int)\n",
    "        ranges = []\n",
    "        for idx, interv in enumerate(intervals[:-1]):\n",
    "            ranges.append((interv, intervals[idx + 1]))\n",
    "    \n",
    "        if sample_type == 'random':\n",
    "            indices = []\n",
    "            for x in ranges:\n",
    "                if x[0] == x[1]:\n",
    "                    indices.append(x[0])\n",
    "                else:\n",
    "                    indices.append(rnd.choice(range(x[0], x[1])))\n",
    "        elif sample_type == 'uniform':\n",
    "            indices = [(x[0] + x[1]) // 2 for x in ranges]\n",
    "        \n",
    "        selected_frames = [frame_files[i] for i in indices]\n",
    "        if len(selected_frames) < sample_num:\n",
    "            selected_frames += [frame_files[-1]] * (sample_num - len(selected_frames))\n",
    "            indices += [indices[-1]] * (sample_num - len(indices))\n",
    "    else:\n",
    "        selected_frames = [frame_files[i] for i in given_index]\n",
    "        indices = given_index\n",
    "    \n",
    "    # [:max_num_frames]\n",
    "    frames = []\n",
    "    # print(len(selected_frames))\n",
    "    for frame_name in selected_frames:\n",
    "        image = Image.open(os.path.join(video_path, frame_name)).convert(\"RGB\")\n",
    "        image = image.resize((WIDTH, HEIGHT), resample=Image.BILINEAR)\n",
    "        frames.append(image)\n",
    "\n",
    "    frames = np.stack(frames, axis=2)\n",
    "    frames = torch.from_numpy(frames).permute(2, 3, 0, 1).contiguous() #.unsqueeze(0)\n",
    "    frames = frames.float().div(255).clamp(0, 1).half() * 2.0 - 1.0\n",
    "    return frames, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self,frame_num=16):\n",
    "        super(EvalDataset, self).__init__()\n",
    "        # self.data = json.load(open('/nas-hdd/shoubin/videos/rovi/data/v4_test.json'))[:100]\n",
    "        # self.video_base_path = '/nas-hdd/shoubin/videos/rovi/data/JPEGImages/'\n",
    "        # self.mask_base_path = '/nas-hdd/shoubin/videos/rovi/data/Annotations/'\n",
    "        # self.frame_num = frame_num\n",
    "        # # self.tokenizer, self.multimodal_cfg = tokenizer, multimodal_cfgs\n",
    "        # print('--num data: %d--'%(len(self.data)))\n",
    "        self.data = json.load(open('/nas-hdd/shoubin/videos/rovi/data/advegas_benchmark.json'))\n",
    "        # self.data = json.load(open('/nas-hdd/shoubin/videos/rovi/data/v4_train.json'))\n",
    "        self.video_base_path = '/nas-hdd/shoubin/videos/rovi/data/JPEGImages/'\n",
    "        # self.mask_base_path = '/nas-hdd/shoubin/videos/rovi/data/Annotations/'\n",
    "        self.mask_base_path = '/nas-hdd/shoubin/advegas/predicted_mask/'\n",
    "        self.inpainted_base_path = '/nas-hdd/shoubin/videos/rovi/data/InpaintImages/'\n",
    "        self.frame_num = frame_num\n",
    "        # self.tokenizer, self.multimodal_cfg = tokenizer, multimodal_cfgs\n",
    "        print('--num data: %d--'%(len(self.data)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        anno = self.data[i]\n",
    "        vid = anno['vid']\n",
    "        task = anno['task']\n",
    "        mask_id = anno['mask_id']\n",
    "        \n",
    "        # if not os.path.exists('/nas-hdd/shoubin/advegas/predicted_boxes/' + vid + '-' + str(mask_id)):\n",
    "        #     return None\n",
    "        # if task!= 'adding':\n",
    "        #     return None\n",
    "        # # print(vid)\n",
    "        \n",
    "        if task == 'removal':\n",
    "            text = 'inpainted background' #anno['prompt'] #'inpainted background'\n",
    "            target, index = load_video(os.path.join(self.inpainted_base_path, vid, mask_id), sample_num=self.frame_num)\n",
    "            condition, _ = load_video(os.path.join(self.video_base_path, vid), sample_num=self.frame_num, given_index=index)\n",
    "            # condition, _ = load_video(os.path.join(self.inpainted_base_path, vid, '3'), sample_num=self.frame_num, given_index=index)\n",
    "            # mask = load_mask(os.path.join(self.mask_base_path, vid), index, mask_id, convert_to_box=False)\n",
    "            mask = load_mask(os.path.join(self.mask_base_path, vid+'-'+str(mask_id)), index, mask_id, convert_to_box=False)\n",
    "            \n",
    "        elif task == 'adding':\n",
    "            text = anno['description'] # anno['prompt']\n",
    "            target, index = load_video(os.path.join(self.video_base_path, vid), sample_num=self.frame_num)\n",
    "            condition, _ = load_video(os.path.join(self.inpainted_base_path, vid, mask_id), sample_num=self.frame_num, given_index=index)\n",
    "            mask = load_mask(os.path.join(self.mask_base_path, vid+'-'+str(mask_id)), index, mask_id, convert_to_box=True)\n",
    "        \n",
    "        elif task == 'editing':\n",
    "            text = anno['prompt']\n",
    "            target, index = load_video(os.path.join(self.video_base_path, vid), sample_num=self.frame_num)\n",
    "            condition, _ = load_video(os.path.join(self.video_base_path, vid), sample_num=self.frame_num, given_index=index)\n",
    "            # condition, _ = load_video(os.path.join(self.inpainted_base_path, vid, '3'), sample_num=self.frame_num, given_index=index)\n",
    "            # mask = load_mask(os.path.join(self.mask_base_path, vid), index, mask_id, convert_to_box=False)\n",
    "            # mask = load_mask(os.path.join(self.mask_base_path, vid), index, mask_id, convert_to_box=True)\n",
    "            mask = load_mask(os.path.join(self.mask_base_path, vid+'-'+str(mask_id)), index, mask_id, convert_to_box=False)\n",
    "            \n",
    "        data_dict = {}\n",
    "        data_dict['task'] = task\n",
    "        data_dict['target'] = target # [1, 8, 3, 320, 512]\n",
    "        data_dict['condition'] = condition # [1, 8, 3, 320, 512]\n",
    "        data_dict['mask'] = mask    # [1, 8, 3, 320, 512]\n",
    "        data_dict['text_prompt'] = text\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvalDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_dict = {2: 'Orange', 5: 'Surfer in red', 8: 'Snowboarder in white', 11: 'Glass Robin',\n",
    "14: 'Man in white shirt', 17: 'Yellow Magpie Bird', 20: 'Wooden Eagle',23: 'Corgi',\n",
    "26: 'Rock Climber in blue', 29: 'Airplane in red', 32: 'Cat', 35: 'Man',\n",
    "38: 'White Dog', 41: 'Man in black', 44: 'Belaying Man in Red', 47: 'Black Poodle',\n",
    "50: 'Dog Handler as Superman', 53: 'Dalmatian', 56: 'Brown Bear', 59:'Orange Shark',\n",
    "62: 'Man wearing a brown leather jacket', 65: 'Man in orange', 68: 'The baby in a white top', 71:'Iron Dog',\n",
    "74: 'Black Dog', 77: 'Skier in white T-shirt', 80: 'A white vehicle', 83:'A baby in a bright red',\n",
    "86: 'Green Snake', 89: 'Robot\\'s Hand', 92: 'Woman in dark red dress', 95:'White Cat',\n",
    "98: 'Player with Blue Hat', 101: 'Observer in white shirt', 104: 'BMX Rider in red', 107:'Panda',\n",
    "110: 'blue-colored dog', 113: 'Spiderman', 116: 'Black Seagull', 119:'Person in rainbow-striped jacket',\n",
    "122: 'White Cat', 125: 'Black Sheep', 128: 'Van Gogh Sheep', 131:'White Dog',\n",
    "134: 'blue and white Stork', 137: 'Superman', 140: 'Runner in Red Shirt', 143:'wooden seagull',\n",
    "146: 'White Camel', 149: 'Basketball Player in a red jersey', 152: 'White Shark', 155:'Black Bird of Prey',\n",
    "158: 'Stone Bonsai', 161: 'woman in red t-shirt', 164: 'Basketball Player in a blue shirt and white shorts', 167:'Glass Dog',\n",
    "170: 'Raccoon', 173: 'White Cat', 176: 'White Bird', 179:'White Bird',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eval_dataset)):\n",
    "    input_dict = eval_dataset[i]\n",
    "    # if input_dict is None:\n",
    "    #     continue\n",
    "    # print(i, input_dict['task'])\n",
    "    if input_dict['task']=='adding':\n",
    "        continue\n",
    "\n",
    "    task = [input_dict['task']]\n",
    "    video = input_dict['condition'].to(weight_dtype).unsqueeze(0).to('cuda:0') # [16, 3, 320, 512]\n",
    "    mask = input_dict['mask'].to(weight_dtype).unsqueeze(0).to('cuda:0') # [16, 1, 320, 512]\n",
    "    # video = video[:,:16,:,:,:]\n",
    "    # mask = mask[:,:16,:,:,:]\n",
    "    text = [input_dict['text_prompt']]\n",
    "    # text = [short_dict[i]]\n",
    "    inpainted = model.inpaint(\n",
    "        video=video, # input video condition\n",
    "        mask=mask,\n",
    "        prompt=text,\n",
    "        task = task)\n",
    "    # print(inpainted.shape)\n",
    "    # output_path = \"/nas-hdd/shoubin/advegas/ablation/adding_wo_detailed/{}.gif\".format(str(i))\n",
    "    # output_path = \"/nas-hdd/shoubin/advegas/ablation/pred_mask/{}/{}.gif\".format(input_dict['task'], str(i))\n",
    "    output_path = './reproduce.gif'\n",
    "    save_videos_grid(inpainted, output_path)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 12236\n",
    "input_dict = eval_dataset[index]\n",
    "print(input_dict.keys())\n",
    "print(input_dict['task'])\n",
    "task = [input_dict['task']]\n",
    "video = input_dict['condition'].to(weight_dtype).unsqueeze(0).to('cuda:1') # [16, 3, 320, 512]\n",
    "mask = input_dict['mask'].to(weight_dtype).unsqueeze(0).to('cuda:1') # [16, 1, 320, 512]\n",
    "# text = ['Giant Panda: The central figure in the video, a giant panda with distinct black and white fur, showcases its climbing skills. It has a large, round body, a characteristic bear-like face with black patches around its eyes, ears, and limbs.']\n",
    "text = [input_dict['text_prompt']]\n",
    "print(text)\n",
    "# print(text)\n",
    "print(video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266bdcaa934f4a30ba99b7599112b7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 320, 512])\n"
     ]
    }
   ],
   "source": [
    "# text ='Kongfu Panda: The Panda is focused on performing dribbling exercises, showcasing control and agility.'\n",
    "# text = ['Corgi: A Corgi dog with a golden coat is walking alongside a larger dog on a sandy beach. The Corgi, smaller and more agile, trots energetically, occasionally darting forward']\n",
    "text = ['Woman: A casual woman in a white beach dress and with a straw hat, walking beside a man, likely accompanying him and his white dog. She moves with a relaxed stride, suggesting a leisurely outing together.']\n",
    "inpainted = model.inpaint(\n",
    "    video=video, # input video condition\n",
    "    mask=mask,\n",
    "    prompt=text,\n",
    "    task = task)\n",
    "print(inpainted.shape)\n",
    "output_path = \"teaser_adding_2.gif\"\n",
    "save_videos_grid(inpainted, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nas-hdd/shoubin/advegas/ours_sd2_50ep/editing.gif'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3fd96c5267\n",
      "dict_keys(['task', 'target', 'condition', 'mask', 'text_prompt'])\n",
      "['Surfer: An athlete wearing a dark wetsuit, possibly black or navy, showcasing talent in balancing and steering on the waves. Their stance is wide and steady, knees bent, arms outstretched for balance, and their posture exuding confidence.']\n"
     ]
    }
   ],
   "source": [
    "index = 28\n",
    "input_dict = eval_dataset[index]\n",
    "print(input_dict.keys())\n",
    "# print(input_dict['task'])\n",
    "# video = input_dict['condition'].to(weight_dtype).unsqueeze(0).to('cuda:0') # [16, 3, 320, 512]\n",
    "mask = input_dict['mask'].to(weight_dtype).unsqueeze(0).to('cuda:0') # [16, 1, 320, 512]\n",
    "# text = ['Giant Panda: The central figure in the video, a giant panda with distinct black and white fur, showcases its climbing skills. It has a large, round body, a characteristic bear-like face with black patches around its eyes, ears, and limbs.']\n",
    "# text = [input_dict['text_prompt']]\n",
    "\n",
    "print(text)\n",
    "# print(text)\n",
    "# print(video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 1, 320, 512])\n",
      "torch.Size([1, 1, 1, 320, 512])\n",
      "torch.Size([1, 16, 1, 320, 512])\n"
     ]
    }
   ],
   "source": [
    "mask_ = mask[:,8,:,:,:].unsqueeze(1)\n",
    "print(mask.shape)\n",
    "print(mask_.shape)\n",
    "fixed_mask = torch.repeat_interleave(mask_, 16, dim=1)\n",
    "print(fixed_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "def create_gif(image_folder, output_path, num_samples=16):\n",
    "    # 获取所有图片文件的路径\n",
    "    WIDTH = 512\n",
    "    HEIGHT = 320\n",
    "    images = sorted([os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith('.jpg')])\n",
    "    \n",
    "    # 计算间隔\n",
    "    step = len(images) // num_samples\n",
    "    \n",
    "    # 选择均匀间隔的图片\n",
    "    selected_images = images[::step][:num_samples]\n",
    "    \n",
    "    # 读取图片\n",
    "    frames = [Image.open(img).resize((WIDTH, HEIGHT), resample=Image.BILINEAR) for img in selected_images]\n",
    "    \n",
    "    # 将图片保存为 GIF\n",
    "    imageio.mimsave(output_path, frames, 'GIF', duration=1000 * (1 / 8), loop=0)  # duration 控制帧之间的时间间隔\n",
    "\n",
    "# 使用函数\n",
    "# image_folder = '/nas-hdd/shoubin/videos/rovi/data/JPEGImages/b24fe36b2a'\n",
    "# output_path = 'orginal.gif'\n",
    "# create_gif(image_folder, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversed_latents = model.DDIM(\n",
    "#         video=video, # input video condition\n",
    "#         mask=mask,\n",
    "#         prompt='',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inv_latents_path = '/nas-hdd/shoubin/advegas/davis_ddim/test.pt'\n",
    "# torch.save(inversed_latents, inv_latents_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 1\n",
    "# input_dict = eval_dataset[index]\n",
    "# print(input_dict.keys())\n",
    "# # video = input_dict['video'].to(weight_dtype).unsqueeze(0).to('cuda:1') # [16, 3, 320, 512]\n",
    "# mask = input_dict['mask'].to(weight_dtype).unsqueeze(0).to('cuda:1') # [16, 1, 320, 512]\n",
    "# text = [input_dict['text_prompt']]\n",
    "# # print(text)\n",
    "# # text = ['change the man to the background']\n",
    "# # print(text)\n",
    "# # print(video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversed_latents[-1].shape\n",
    "# len(inversed_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414e1bf77def4f25872ff9034a308b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# text = ['Giant Panda: The central figure in the video, a giant panda with distinct black and white fur, showcases its climbing skills.']\n",
    "text = ['White Bird: A bird is flying, its fur is pure white.']\n",
    "reconstruct = model.inpaint(\n",
    "        video=video, # input video condition\n",
    "        mask=fixed_mask,\n",
    "        prompt=text,\n",
    "        task = task\n",
    "        # latents=inversed_latents[-1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = reconstruct #pipe_output.videos\n",
    "output_path = \"v4_4000_{}_adding_fixed_mask3.gif\".format(str(index))\n",
    "save_videos_grid(reconstruct, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = (video / 2 + 0.5).clamp(0, 1)\n",
    "# # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n",
    "# video = video.cpu().float().numpy()\n",
    "# video = torch.from_numpy(video)\n",
    "# video=video.permute(0,2,1,3,4)\n",
    "# output_path = \"in.gif\"\n",
    "# save_videos_grid(video, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
